---
title: "Exponential Smoothing in R"
author: "Salma Elshahawy"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Exponential Smoothing in R}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  echo = FALSE,
  comment = "#>",
  fig.width = 7,
  fig.height = 4
)
library(fpp2)
library(forecast)
library(readr)
library(stringr)
library(tidyverse)
library(ggplot2)
library(gridExtra)
theme_set(theme_classic())
```

## Exponential Smoothing and Innovation State Space Model (ISSM)


Exponential smoothing (ETS, which stands for Error, Trend, and Seasonality) is a family of very successful forecasting methods which are based on the key property that forecasts are weighted combinations of past observations (Hyndman et. al, 2008). For example, in simple exponential smoothing, the forecast $\hat{z}_{T+1}$ for time step $T+1$ is written as

$$\hat{z}_{T+1} = \hat{z}_T + \alpha (z_T - \hat{z}_T) = \alpha\cdot z_T + (1 - \alpha)\cdot \hat{z}_T,$$
Here $\alpha > 0$  is a smoothing parameter that controls the weight given to each observation. Note that the recent observations are given more weight than the older observations. In fact the weight given to the past observation decreases exponentially as it gets older and hence the name **exponential smoothing**.

As discussed already, whether we use additive or multiplicative errors, the point forecasts will be the same and the prediction intervals will differ. To distinguish models, we add an extra letter to the method notation. The triplet ETS (⋅,⋅,⋅)  refers to error, trend, and seasonality (in that order)

General exponential smoothing methods consider the extensions of simple ETS to include time series patterns such as (linear) trend, various periodic seasonal effects. All ETS methods falls under the category of forecasting methods as the predictions are point forecasts (a single value is predicted for each future time step). On the other hand a statistical model describes the underlying data generation process and has an advantage that it can produce an entire probability distribution for each of the future time steps. Innovation state space model (ISSM) is an example of such models with considerable flexibility in representing commonly occurring time series patterns and underlie the exponential smoothing methods.

The idea behind ISSMs is to maintain a latent state vector $l_{t}$ with recent information about level, trend, and seasonality factors. The state vector $l_{t}$ evolves over time adding small innvoation (i.e., the Gaussian noise) at each time step. The observations are then a linear combination of the components of the current state.

Mathematically, ISSM is specified by 2 equations,

- The state transition equation:
$$l_{t} = F_t l_{t-1} + g_{t}\epsilon_t,\quad \epsilon_t\sim \mathcal{N}(0,1).$$
Note that the innovation strength is controlled by $g_t$


- The observation equation:

$$z_t = a_{t}^{\top}l_{t-1} + b_t + \nu_t, \quad \nu_t \sim \mathcal{N}(0, \sigma_t^2)$$

Here we allow for an additional term $b_t$ which can model any determinstic component (exogenous variables).
This describes a fairy generic model allowing the user to encode specific time series patterns using the coefficients $F, a_t$ and thus are problem dependent. 

The innovation vector $g_t$ comes in terms of parameters to be learned (the innovation strengths). Moreover, the initial state $l_0$ has to be specified. We do so by specifying a Gaussian prior distribution $P(l_0)$, whose parameters (means, standard deviation) are learned from data as well.

The parameters of the ISSM are typically learned using the maximum likelihood principle. This requires the computation of the log-likelihood of the given observations i.e., computing the probability of the data under the model, $P(z_1, ..., z_t)$

----

## Specificying the model type

So when specificying the model type you always specificy the error, trend, then seasonality (hence “ets”). The options you can specify for each component is as follows:

  - error: additive (“A”), multiplicative (“M”), unknown (“Z”)
  
  - trend: none (“N”), additive (“A”), multiplicative (“M”), unknown (“Z”)
  
  - seasonality: none (“N”), additive (“A”), multiplicative (“M”), unknown (“Z”)


Consequently, if we wanted to apply a Holt’s model where the error and trend were additive and no seasonality exists we would select `model = "AAN"`. 

If you want to apply a Holt-Winters model where there is additive error, an exponential (multiplicative) trend, and additive seasonality you would select `model = "AMA"`. 

If you are uncertain of the type of component then you use “Z”. So if you were uncertain of the components or if you want the model to select the best option, you could use `model = "ZZZ"` and the “optimal” model will be selected.


## Example

We will use the `qcement` passengers dataset within the `fpp2` package for illustration. The first step we do is by splitting the `qcement` dataset into train and test set to compare performance. This data has seasonality and trend; however, it is unclear if seasonality is additive or multiplicative. 

```{r}
# create training and validation of the AirPassengers data
qcement.train <- window(qcement, end = c(2012, 4))
qcement.test <- window(qcement, start = c(2013, 1))
```


```{r}
qcement.hw <- ets(qcement.train, model = "AAA")  #stands for a model with additive error, additive trend, and additve seasonality.
summary(qcement.hw)

```

If we assess our additive model we can see that $a = 0.6418$, $\beta = 0.0001$, and $\gamma = 0.1988$

> **The important thing to understand about the `ets()` model is how to select the
`model = parameter`. In total we have `36 model` options to choose from.** 

With the `ets()` function, the default estimation method is **maximum likelihood rather than minimum sum of squares**.


```{r}
autoplot(qcement.hw)
```

```{r}
autoplot(forecast(qcement.hw))
```


If we check our residuals, we see that residuals grow larger over time. This may suggest that a multiplicative error rate may be more appropriate.


```{r}
checkresiduals(qcement.hw)
```


```{r}
# cbind('Residuals' = residuals(fit),
#       'Forecast errors' = residuals(fit,type='response')) %>%
#   autoplot(facet=TRUE) + xlab("Year") + ylab("")
```

------

### Compare predictive types, accuracy prespective

To compare the predictive accuracy of our models let’s compare four different models. We see that the first model (additive error, trend and seasonality) results in the lowest RMSE and MAPE on test test data set.

```{r}
# additive error, trend and seasonality
qcement.hw1 <- ets(qcement.train, model = "AAA")
# forecast the next 5 quarters
qcement.f1 <- forecast(qcement.hw1, h = 5)

# check accuracy
accuracy(qcement.f1, qcement.test)
```



```{r}
# multiplicative error, additive trend and seasonality
qcement.hw2 <- ets(qcement.train, model = "MAA")
qcement.f2 <- forecast(qcement.hw2, h = 5)
accuracy(qcement.f2, qcement.test)
```

```{r}
# additive error and trend and multiplicative seasonality
qcement.hw3 <- ets(qcement.train, model = "AAM", restrict = FALSE)
qcement.f3 <- forecast(qcement.hw3, h = 5)
accuracy(qcement.f3, qcement.test)

```


```{r}
# multiplicative error, additive trend, and multiplicative seasonality
qcement.hw4 <- ets(qcement.train, model = "MAM")
qcement.f4 <- forecast(qcement.hw4, h = 5)
accuracy(qcement.f4, qcement.test)
```

If we were to compare this to an unspecified model where we let `ets` select the optimal model, we see that `ets` selects a model specification of multiplicative error, additive trend, and multiplicative seasonality (“MAM”). This is equivalent to our fourth model above. This model is assumed “optimal” because it minimizes RMSE, AIC, and BIC on the training data set, but does not necessarily minimize prediction errors on the test set.

```{r}
qcement.hw5 <- ets(qcement.train, model = "ZZZ")
summary(qcement.hw5)
```

We can optimize the $\gamma$ parameter in our Holt-Winters model (if we don't want to use the triple z option). Here, we use the additive error, trend and seasonality model that minimized our prediction errors above and identify the $\gamma$ parameter that minimizes forecast errors. In this case we see that $\gamma = 0.21$ minimizes the error rate.


```{r}
gamma <- seq(0.01, 0.85, 0.01)
RMSE <- NA

for(i in seq_along(gamma)) {
  hw.expo <- ets(qcement.train, "AAA", gamma = gamma[i])
  future <- forecast(hw.expo, h = 5)
  RMSE[i] = accuracy(future, qcement.test)[2,2]
}

error <- data_frame(gamma, RMSE)
minimum <- filter(error, RMSE == min(RMSE))
ggplot(error, aes(gamma, RMSE)) +
  geom_line() +
  geom_point(data = minimum, color = "blue", size = 2) +
  ggtitle("gamma's impact on forecast errors",
          subtitle = "gamma = 0.21 minimizes RMSE")
```

If we update our model with this “optimal” $\gamma$ parameter we see that we bring our forecasting error rate down from 2.88% to 2.76%. This is a small improvement, but often small improvements can have large business implications.

```{r}
# previous model with additive error, trend and seasonality
accuracy(qcement.f1, qcement.test)
```

```{r}
# new model with optimal gamma parameter
qcement.hw6 <- ets(qcement.train, model = "AAA", gamma = 0.21)
qcement.f6 <- forecast(qcement.hw6, h = 5, level =c(80, 95))
accuracy(qcement.f6, qcement.test)
```

With this new optimal model we can get our predicted values:

```{r}
qcement.f6
```

```{r}
autoplot(qcement.f6) 
```


