---
title: "project_1"
author: "Salma Elshahawy"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Homework 6 Predictive analytics}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
[Github repo](https://github.com/salma71/MSDS_spring2021/tree/master/data_624) | [portfolio](https://salma71.github.io/) | [Blog](https://salmaeng71.medium.com/)

```{r setup, include = FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	fig.height = 3,
	fig.width = 7,
	message = FALSE,
	warning = FALSE,
	collapse = TRUE,
	comment = "#>"
)
library(tidyverse)
library(lubridate)
library(kableExtra)
library(readxl)
library(fpp2)
library(forecast)
library(gridExtra)
library(openxlsx)
library(tseries)
library(scales)
theme_set(theme_classic())
```


## Part A – ATM Forecast, ATM624Data.xlsx

[The generated forcates are on Github](https://github.com/salma71/MSDS_spring2021/blob/master/data_624/ATM_Forecasts.xlsx)

In part A, I want you to forecast how much cash is taken out of 4 different ATM machines for May 2010.  The data is given in a single file.  The variable ‘Cash’ is provided in hundreds of dollars, other than that it is straight forward.   I am being somewhat ambiguous on purpose to make this have a little more business feeling.  Explain and demonstrate your process, techniques used and not used, and your actual forecast.  I am giving you data via an excel file, please provide your written report on your findings, visuals, discussion and your R code via an RPubs link along with the actual.rmd file  Also please submit the forecast which you will put in an Excel readable file.


## Introduction

For this part, we need to forecast how much cash will be taken out of 4 different ATM machines for May 2010.  
Data was provided for this project.  The cash is in hundreds of dollars.

## Data Exploration

I will begin by plotting the cash withdrawn by each ATM.

```{r}
atm <- read_excel("ATM624Data.xlsx") %>%
  mutate(DATE = as.Date(DATE, origin = "1899-12-30"),
         ATM = as.factor(ATM)) %>%
  data.frame()
```

```{r}
atm %>%
  ggplot(aes(DATE, Cash, color = ATM)) +
  geom_line() +
  ggtitle("Cash Withdrawal by Date and ATM") +
  scale_color_brewer(palette = "Set1") +
  scale_y_continuous(labels = comma) +
  facet_wrap(.~ATM, ncol = 2) +
  theme(legend.title = element_blank(),
        legend.position = "bottom",
        axis.title = element_blank())
```

ATM4 was used at a higher rate than every other ATM in the data series.  There is an unusual spike in ATM4. There are some missing ATM labels in the data. Let's remove ATM4 and the missing label from the dataset and replot the visualization:

```{r}
atm %>%
  filter(ATM != "ATM4") %>%
  filter(!is.na(ATM)) %>%
  ggplot(aes(DATE, Cash, color = ATM)) +
  geom_line() +
  facet_wrap(.~ATM, ncol = 2) +
  ggtitle("Cash Withdrawal by Date and ATM") +
  scale_color_brewer(palette = "Set1") +
  theme(legend.title = element_blank(),
        legend.position = "bottom",
        axis.title = element_blank())
```

It seems that ATM3 was not used until recently.  ATM #1 and #2 seem similar to each other.  It doesn't seem like there is much of a trend in the data.  Let's explore the distributions further:

```{r}
atm %>%
  na.omit() %>%
  ggplot(aes(ATM, Cash, color = ATM)) +
  geom_boxplot() +
  ggtitle("Cash Withdrawal by ATM") +
  scale_color_brewer(palette = "Set1") +
  facet_wrap(.~ATM, nrow = 2, scales = "free") +
  theme(legend.position = "none",
        axis.title.y = element_blank(),
        axis.title.x = element_blank(),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank())
atm %>%
  filter(!is.na(ATM)) %>%
  group_by(ATM) %>%
  summarise(Minimum = min(Cash, na.rm = T),
            `1st Qu.` = quantile(Cash, .25, na.rm = T),
            Mean = mean(Cash, na.rm = T),
            Median = median(Cash, na.rm = T),
            `3rd Qu.` = quantile(Cash, .75, na.rm = T),
            Maximum = max(Cash, na.rm = T),
            `NA's` = sum(is.na(Cash))) %>%
  kable() %>%
  kable_styling()
```


There is a pattern in the time series but fluctuations are very tight.  I wonder if it is explained by the day of the week.  For example, Friday and Saturday might have heavier usage and a Tuesday night might be calm.  Let's examine this hypothesis:

```{r}
atm %>%
  na.omit() %>%
  mutate(`Day of Week` = recode(as.factor(wday(DATE)), "1" = "Sunday", "2" = "Monday", "3" = "Tuesday", "4" = "Wednesday", "5" = "Thursday", "6" = "Friday", "7" = "Saturday"))  %>%
  ggplot(aes(`Day of Week`, Cash, color = `Day of Week`)) +
  geom_boxplot() +
  facet_grid(ATM ~ `Day of Week`, scales = "free") +
  scale_color_brewer(palette = "Set1") +
  theme(legend.position = "none", axis.title = element_blank(), axis.text.x = element_blank(), axis.ticks = element_blank())
```

It looks like there is some varriation based on the day of the week. Thursday looks to be less busy than the other days of the week.  There is so much variation between the 4 ATMs that each will be modeled separately.

We will clean up the data to produce the forecasts.  Then explore some candidate modeling techniques.  Once we have a handful of candidates, we will prefrom a cross-validation on the models and get the RMSE.  The model that minimizes the RMSE during cross-validation will be selected as the model of choice.

### ATM_1

#### Data Cleanup

There are 3 missing observations that need to be cleaned up.

```{r}
atm1 <- atm %>%
  filter(ATM == "ATM1")
atm1$clean_cash <- c(tsclean(ts(atm1$Cash, start = decimal_date(as.Date(min(atm1$DATE))), frequency = 365)))
```

Now that we have a complete data set I will create time series objects.  Since there is a weekly effect I will be using a frequency of 7.  This does mess up the dates in the plot however so please pay no reguard to them.

```{r}
ggtsplot <- function(ts, title) {
  grid.arrange(
    autoplot(ts) +
      ggtitle(title) +
      scale_y_continuous(labels = comma) +
      theme(axis.title = element_blank()),
    grid.arrange(
      ggAcf(ts) + ggtitle(element_blank()),
      ggPacf(ts) + ggtitle(element_blank()), ncol = 2)
    , nrow = 2)
}
atm1_ts <- ts(atm1$clean_cash, start = decimal_date(as.Date(min(atm1$DATE))), frequency = 7)
ggtsplot(atm1_ts, "ATM #1")
```

One readily observes the repeating peaks on every 7th lag.  There also seems to be an interesting negative correlation between the 1st and both the 3rd and the 5th lag.

### Model Creation 

#### STL Decomposition Models

I will try a couple of seasonal decomposition models.  I will set the seasonal window to 7 so it picks up the day of the week variation.  

```{r}
atm1_ts %>%
  stl(s.window = 7, robust = TRUE) %>%
  autoplot()
```

This seems promising to me.  I will do STL decomposition forecasts using both the ETS and ARIMA models and check their residual plots

#### STL + ETS

```{r}
h <- 31
atm1_stl_ets_fit <- atm1_ts %>%
  stlf(h = h, s.window = 7, robust = TRUE, method = "ets")
checkresiduals(atm1_stl_ets_fit)
```

This model seems to hold merit and should be taken under consideration.

#### STL + ARIMA

```{r}
atm1_stl_arima_fit <- atm1_ts %>%
  stlf(h = h, s.window = 7, robust = TRUE, method = "arima")
checkresiduals(atm1_stl_arima_fit)
```

This model also preformed well.  It is definately a candidate for the cross-validation stage.

#### Holt-Winters

```{r}
atm1_hw_fit <- hw(atm1_ts, h = h)
checkresiduals(atm1_hw_fit)
```

This method did a fairly good job.  I will move it on to the next phase.

### Holt-Winters with Box Cox Adjustment

```{r}
atm1_lambda <- BoxCox.lambda(atm1_ts)
atm1_adj_hw_fit <- hw(atm1_ts, h = h, lambda = atm1_lambda)
checkresiduals(atm1_adj_hw_fit)
```

This seems to be a strong candidate.  We will see how if fares in the cross-validation.

#### ARIMA

```{r}
atm1_arima_fit <- auto.arima(atm1_ts)
checkresiduals(atm1_arima_fit)
```

This model looks like it is preforming well.  Let's see how all of them stack up.

### Cross Validation

In order to understand how well a model is likely to preform at predicting out of sample data, I will use the `tsCV` function and evaluate the models.  As prevously noted my goal is to minimize the RMSE.  First I will get the errors from the cross validation process, then I will compute the RMSE.

```{r}
get_rmse <- function(e) {
  sqrt(mean(e^2, na.rm = TRUE))
}
atm1_arima_forecast <- function(x, h) {
  forecast(Arima(x, order = c(0, 0, 1), seasonal = c(0, 1, 2)), h = h)
}
stl_ets_errors <- tsCV(atm1_ts, stlf, h = h, s.window = 7, robust = TRUE, method = "ets")
stl_arima_errors <- tsCV(atm1_ts, stlf, h = h, s.window = 7, robust = TRUE, method = "arima")
hw_errors <- tsCV(atm1_ts, hw, h = h)
adj_hw_errors <- tsCV(atm1_ts, hw, h = h, lambda = atm1_lambda)
arima_errors <- tsCV(atm1_ts, atm1_arima_forecast, h = h)
data.frame(Model = c("STL ETS", "STL ARIMA", "ARIMA", "Holt-Winters", "Adjusted Holt-Winters"),
           RMSE = c(get_rmse(stl_ets_errors[, h]), get_rmse(stl_arima_errors[, h]), get_rmse(arima_errors[, h]), get_rmse(hw_errors[, h]), get_rmse(adj_hw_errors[, h]))) %>%
  arrange(RMSE) %>%
  kable() %>%
  kable_styling()
```


It looks like the ARIMA model perform good job predicting on the out of sample data set.  

### Model Selection

The ARIMA model would be selected to produce the forecast for ATM #1.  This because it is the model has more generalization and robustness across the cross validation.

----

### ATM 2

We will repeat the same process for ATM 2, then select the most generalized robust model using the cross validation.

### Data Cleanup

There are 2 missing observations.  This will be cleaned up using the `tsclean` function again.

```{r}
atm2 <- atm %>%
  filter(ATM == "ATM2")
atm2$clean_cash <- c(tsclean(ts(atm2$Cash, start = decimal_date(as.Date(min(atm2$DATE))), frequency = 365)))
atm2_ts <- ts(atm2$clean_cash, start = decimal_date(as.Date(min(atm2$DATE))), frequency = 7)
ggtsplot(atm2_ts, "ATM #2")
```

Once again the ACF plot has the regular spikes on evry multiple of 7.

### Model Creation 

#### STL Decomposition Models

Again I will try a couple of seasonal decomposition models.  I will set the seasonal window to 7 so it picks up the day of the week variation.  

```{r}
atm2_ts %>%
  stl(s.window = 7, robust = TRUE) %>%
  autoplot()
```

This seems very similar to ATM #1.

#### STL + ETS

```{r}
atm2_stl_ets_fit <- atm2_ts %>%
  stlf(h = h, s.window = 7, robust = TRUE, method = "ets")
checkresiduals(atm2_stl_ets_fit)
```

This model preformed much better than the it did for the ATM #1 time series.  It will be interesting to see how it does in cross-validation

#### STL + ARIMA

```{r}
atm2_stl_arima_fit <- atm2_ts %>%
  stlf(h = h, s.window = 7, robust = TRUE, method = "arima")
checkresiduals(atm2_stl_arima_fit)
```

Interesting.  The residuals have a bit more spread but the left tail is shorter than the STL+ETS model.

#### Holt-Winters

```{r}
atm2_hw_fit <- hw(atm2_ts, h = h)
checkresiduals(atm2_hw_fit)
```

This method did not preform as well as the others.  I will, however keep it in so I can compare it to ATM #1's statistics.

### Holt-Winters with Box Cox Adjustment

```{r}
atm2_lambda <- BoxCox.lambda(atm2_ts)
atm2_adj_hw_fit <- hw(atm2_ts, h = h, lambda = atm2_lambda)
checkresiduals(atm2_adj_hw_fit)
```

#### ARIMA

```{r}
atm2_arima_fit <- auto.arima(atm2_ts)
checkresiduals(atm2_arima_fit)
```

This model looks like it is preforming well.  Let's see how all of them stack up.

### Cross Validation

In order to understand how well a model is likely to preform at predicting out of sample data, I will use the `tsCV` function and evaluate the models.  As previously noted my goal is to minimize the RMSE.  First I will get the errors from the cross validation process, then I will compute the RMSE.

```{r}
atm2_arima_forecast <- function(x, h) {
  forecast(Arima(x, order = c(2, 0, 2), seasonal = c(0, 1, 1)), h = h)
}
stl_ets_errors <- tsCV(atm2_ts, stlf, h = h, s.window = 7, robust = TRUE, method = "ets")
stl_arima_errors <- tsCV(atm2_ts, stlf, h = h, s.window = 7, robust = TRUE, method = "arima")
hw_errors <- tsCV(atm2_ts, hw, h = h)
adj_hw_errors <- tsCV(atm2_ts, hw, h = h, lambda = atm2_lambda)
arima_errors <- tsCV(atm2_ts, atm2_arima_forecast, h = h)
data.frame(Model = c("STL ETS", "STL ARIMA", "ARIMA", "Holt-Winters", "Adjusted Holt-Winters"),
           RMSE = c(get_rmse(stl_ets_errors[, h]), get_rmse(stl_arima_errors[, h]), get_rmse(arima_errors[, h]), get_rmse(hw_errors[, h]), get_rmse(adj_hw_errors[, h]))) %>%
  arrange(RMSE) %>%
  kable() %>%
  kable_styling()
```

Interesting.  It looks like the ARIMA model was the top preformer again.  I find it interesting that the RMSE is higher and more spread out for ATM #2.  ATM #1's RMSEs ranged from roughly ~ 30 to 39.  The RMSEs for ATM #2 ranged from ~ 33 to 55.  There is more variability in this data that isn't captured by the model.

### Model Selection

As stated in the previous section I will use the ARIMA model to produce the forecast for ATM #2.

## ATM #3

This model is quite different from the two proceeding cases.  You can see it in the plot: 

```{r}
atm %>%
  filter(ATM == "ATM3") %>%
  mutate(nonzero = if_else(Cash == 0, "No", "Yes")) %>%
  ggplot(aes(DATE, Cash, color = nonzero)) +
  geom_point() +
  ggtitle("ATM #3") +
  scale_color_brewer(palette = "Set1") +
  theme(axis.title = element_blank(), legend.position = "none")
```

Most of the values are zeros (points in red above), except for 3 points (shown in blue).  The three non-zero points are the most current.  This presents a serious challenge.

### Model Creation/Selection

There is one fundamental question with this dataset.  Are the three points an outlier, or is it the begining of the new normal?  If they are outliers one would expect the cash value to return to zero.  If the three points are an indication of change, then the historical data have little relevance.

I will be assuming the new data are the begining of the new normal.  The challenge is we only have three data points.  In the absense of more data I will calculate the average of these three points and use it for the forecast with the recommendation to revise it frequently. As the average is only based off of three data points it should not be considered stable.

```{r}
atm3 <- atm %>%
  filter(ATM == "ATM3", Cash > 0)
atm3_mean <- mean(atm3$Cash)
```

## ATM #4

This ATM is different too, but not as radically different as ATM #3.  I will be using the same approach used for ATM #1 and #2 with this ATM.

### Data Cleanup

There is one major outlier in the data set.  We will clean it up by simply by using the `tsclean` function once again.

```{r}
atm4 <- atm %>%
  filter(ATM == "ATM4")
atm4$clean_cash <- c(tsclean(ts(atm4$Cash, start = decimal_date(as.Date(min(atm4$DATE))), frequency = 365)))
atm4_ts <- ts(atm4$clean_cash, start = decimal_date(as.Date(min(atm4$DATE))), frequency = 7)
ggtsplot(atm4_ts, "ATM #4")
```

Once again there is the familiar patter of peaks on the multiples of seven.

### Model Creation 

#### STL Decomposition Models

Again I will try a couple of seasonal decomposition models.  I will set the seasonal window to 7 so it picks up the day of the week variation.  

```{r}
atm2_ts %>%
  stl(s.window = 7, robust = TRUE) %>%
  autoplot()
```

This seems very similar to ATM #1.

#### STL + ETS

```{r}
atm4_stl_ets_fit <- atm4_ts %>%
  stlf(h = h, s.window = 7, robust = TRUE, method = "ets")
checkresiduals(atm4_stl_ets_fit)
```

This model did a fair job.  There are a couple of ACF spikes that are outside the bands.  Let's see if the ARIMA model does better:

#### STL + ARIMA

```{r}
atm4_stl_arima_fit <- atm4_ts %>%
  stlf(h = h, s.window = 7, robust = TRUE, method = "arima")
checkresiduals(atm4_stl_arima_fit)
```

This is an improvement over the STL+ETS model.  There are still a couple of spikes on the ACF that falls outside the threshold.

#### Holt-Winters

```{r}
atm4_hw_fit <- hw(atm4_ts, h = h)
checkresiduals(atm4_hw_fit)
```

This method seems to have preformed better than any of the STL models.

### Holt-Winters with Box Cox Adjustment

```{r}
atm4_lambda <- BoxCox.lambda(atm4_ts)
atm4_adj_hw_fit <- hw(atm4_ts, h = h, lambda = atm4_lambda)
checkresiduals(atm4_adj_hw_fit)
```

This model preformed very well.  It doesn't look like any of the ACF spikes are outside the bands.  This is a contender for sure.

#### ARIMA

```{r}
atm4_arima_fit <- auto.arima(atm4_ts)
checkresiduals(atm4_arima_fit)
```

This model did fairly well, but it seems like the Holt-Winters with Box-Cox adjustment did better.  It's time to cross-validate and see how all of them preform.

### Cross Validation

Once again, I will use the `tsCV` function and evaluate the models.  My goal to minimize the RMSE remains for this dataset. 

```{r}
atm4_arima_forecast <- function(x, h) {
  forecast(Arima(x, order = c(0, 0, 3), seasonal = c(1, 0, 0)), h = h)
}
stl_ets_errors <- tsCV(atm4_ts, stlf, h = h, s.window = 7, robust = TRUE, method = "ets")
stl_arima_errors <- tsCV(atm4_ts, stlf, h = h, s.window = 7, robust = TRUE, method = "arima")
hw_errors <- tsCV(atm4_ts, hw, h = h)
adj_hw_errors <- tsCV(atm4_ts, hw, h = h, lambda = atm4_lambda)
arima_errors <- tsCV(atm4_ts, atm4_arima_forecast, h = h)
data.frame(Model = c("STL ETS", "STL ARIMA", "ARIMA", "Holt-Winters", "Adjusted Holt-Winters"),
           RMSE = c(get_rmse(stl_ets_errors[, h]), get_rmse(stl_arima_errors[, h]), get_rmse(arima_errors[, h]), get_rmse(hw_errors[, h]), get_rmse(adj_hw_errors[, h]))) %>%
  arrange(RMSE) %>%
  kable() %>%
  kable_styling()
```

That's an unexpected result.  The Adjusted Holt-Winters (the one I thought would preform the best), did not preform as well.  The ARIMA model, once again, rose to the top.

### Model Selection

Once again I will use the forecasts produced by the ARIMA model as the projections for ATM #4.

## Summary

I set out to create predictions for 4 different ATMs.  After testing multiple approaches using cross-validation, I selected ARIMA models for ATM #1, #2 and #4, as it was the modeling technique with the lowest RMSE.  For ATM #3 we used the mean of all non-zero data (3 observations).  This model needs to be updated once more data becomes available. I will finish this project by exporting my forcasts in the same file format as the original data.

```{r, echo=FALSE}
dates <- seq(ymd("2010-05-01"), ymd("2010-05-31"), by = "1 day")
atm1_forecast <- forecast(atm1_arima_fit, h = h)
atm2_forecast <- forecast(atm2_arima_fit, h = h)
atm3_forecast <- rep(atm3_mean, h)
atm4_forecast <- forecast(atm4_arima_fit, h = h)
forecasts_df <- data.frame("DATE" = dates, "ATM" = c("ATM1"), "Cash" = c(atm1_forecast$mean))
forecasts_df <- data.frame("DATE" = dates, "ATM" = c("ATM2"), "Cash" = c(atm2_forecast$mean)) %>%
  rbind(forecasts_df, .)
forecasts_df <- data.frame("DATE" = dates, "ATM" = c("ATM3"), "Cash" = atm3_forecast) %>%
  rbind(forecasts_df, .)
forecasts_df <- data.frame("DATE" = dates, "ATM" = c("ATM4"), "Cash" = c(atm4_forecast$mean)) %>%
  rbind(forecasts_df, .)
write.xlsx(forecasts_df, "ATM_Forecasts.xlsx")
```


-----
-----

## Part B – Forecasting Power, ResidentialCustomerForecastLoad-624.xlsx

[The generated forcates are on Github](https://github.com/salma71/MSDS_spring2021/blob/master/data_624/Residential_Customer_Load_Forecast.xlsx)

Part B consists of a simple dataset of residential power usage for January 1998 until December 2013.  Your assignment is to model these data and a monthly forecast for 2014.  The data is given in a single file.  The variable ‘KWH’ is power consumption in Kilowatt hours, the rest is straight forward.    Add this to your existing files above. 

## Introduction

I am to forecast how much power will be consumed by residential customers in kilowatt hours for the next 12 months.  Data was provided for this project spaning January 1998 until December 2013.  

## Data Exploration

I will begin by reading in the data and ploting the series.

```{r}
raw_data <- read_excel("ResidentialCustomerForecastLoad-624.xlsx")
kwh <- raw_data %>%
  select(KWH) %>%
  ts(start = decimal_date(date("1998-01-01")), frequency = 12)
autoplot(kwh, main = "Residential Power Usage (KWH)") +
  scale_y_continuous(labels = comma) +
  theme(axis.title = element_blank())
```

```{r}
summary(kwh) %>% kable() %>% kable_styling()
```

We have outliers and missing data.  We will clean them up with `tsclean` from the forcasts package, and replot the data with the ACF and PACF:

```{r}
kwh <- tsclean(kwh)
ggtsplot <- function(ts, title) {
  # A ggplot2 version of tsdisplay(df)
  # Args:
  #    ts (Time-Series): The time series we want to plot
  #    title (str): The title of the graph
  grid.arrange(
    autoplot(ts) +
      scale_y_continuous(labels = comma) +
      ggtitle(title) +
      theme(axis.title = element_blank()),
    grid.arrange(
      ggAcf(ts) + ggtitle(element_blank()),
      ggPacf(ts) + ggtitle(element_blank()), ncol = 2)
    , nrow = 2)
}
ggtsplot(kwh, "Cleaned Residential Power Usage (KWH)")
```

This looks much better.  There is a seasonal component that looks to be additive.  I will need to use a model that captures seasonality.

## Candidate Models

Since the data is highly seasonal, I we will need to use an ARIMA or Holt-Winters model.  Let's fit an models using `auto.arima()` and `hw()` and visualize their projections.  I will use a couple of different parameters with the Holt-Winters model.

```{r}
h <- 12
lambda <- BoxCox.lambda(kwh)
autoplot(kwh) +
  autolayer(hw(kwh, h = h), series = "Holt-Winters") +
  autolayer(hw(kwh, h = h, lambda = lambda), series = "Holt-Winters (Box-Cox Adjusted)") +
  autolayer(hw(kwh, h = h, lambda = lambda, damped = TRUE), series = "Holt-Winters (Damped & Box-Cox Adjusted)") +
  autolayer(forecast(auto.arima(kwh), h = h), series = "ARIMA") +
  facet_wrap(. ~ series, ncol = 2) +
  scale_color_brewer(palette = "Set1") +
  scale_y_continuous(labels = comma) +
  theme(legend.position = "none", axis.title = element_blank())
```

## Residual Analysis

All models do a good job picking up the seasonality.  Let's look at the residuals to see if any of the models should not be used.

### Holt-Winters Model

```{r}
hw_fit <- hw(kwh, h = h)
checkresiduals(hw_fit)
```

There are some spikes in the ACF that indicate this is not the best model.

### Adjusted Holt-Winters Model

```{r}
adj_hw_fit <- hw(kwh, h = h, lambda = lambda)
checkresiduals(adj_hw_fit)
```

This is a bit better than the previous model but there are still ACF spikes outside of the bands.

### Damped & Adjusted Holt-Winters Model

```{r}
damp_adj_hw_fit <- hw(kwh, h = h, lambda = lambda, damped = TRUE)
checkresiduals(damp_adj_hw_fit)
```

The preformance of this model is about the same as above.

### ARIMA Model

```{r}
arima_fit <- auto.arima(kwh)
checkresiduals(arima_fit)
```

This model seems to be the best.  There is only one ACF spike outside of the threshold and it only by a small amount.  My working theory is this is the model I should use, but I will check this using cross validation.

## Cross Validation

In order to understand how well a model is likely to preform at predicting out of sample data, I will use the `tsCV` function and evaluate the models.  My goal is to minimize the RMSE.  First I will get the errors from the cross validation process, then I will compute the RMSE.

```{r}
get_rmse <- function(e) {
  sqrt(mean(e^2, na.rm = TRUE))
}
arima_forecast <- function(x, h) {
  forecast(Arima(x, order = c(3, 0, 1), seasonal = c(2, 1, 0), include.drift = TRUE), h = h)
}
hw_error <- tsCV(kwh, hw, h = h)
adj_hw_error <- tsCV(kwh, hw, h = h, lambda = lambda)
damped_adj_hw_error <- tsCV(kwh, hw, h = h, lambda = lambda, damped = TRUE)
arima_errors <- tsCV(kwh, arima_forecast, h = h)
data.frame(Model = c("ARIMA", "Holt-Winters", "Adjusted Holt-Winters", "Damped & Adjusted Holt-Winters"),
           RMSE = c(get_rmse(arima_errors[, h]), get_rmse(hw_error[, h]), get_rmse(adj_hw_error[, h]), get_rmse(damped_adj_hw_error[, h]))) %>%
  arrange(RMSE) %>%
  kable() %>%
  kable_styling()
```

The ARIMA model had the best cross validation results.  

## Model Selection

Given that the ARIMA model minimized the RMSE in the cross validation, I will use it to forecast residential power consumption.

## Summary

I set out to forecast residential power consumption.  Four modeling techniques were tested for accuracy using cross-validation.  In the end an ARIMA model was selected bercause it minimized the RMSE.  Here's my forcast for the next `r h` months of power usage:

```{r}
my_forecast <- forecast(arima_fit, h = h)
autoplot(my_forecast) +
  ggtitle("Forecasted Residential Power Consumption (KWH)") +
  theme(axis.title = element_blank())
my_forecast %>%
  kable() %>%
  kable_styling()
```

```{r, echo=FALSE}
max_case_sequence <- max(raw_data$CaseSequence)
data.frame(KWH = round(my_forecast$mean)) %>%
  mutate(CaseSequence = row_number()) %>%
  rowwise() %>%
  mutate(`YYYY-MMM` = format(ymd(paste0("2014", ifelse(CaseSequence < 10, paste0("0", CaseSequence), CaseSequence), "01")), "%Y-%b")) %>%
  mutate(CaseSequence = CaseSequence + max_case_sequence) %>%
  ungroup() %>%
  select(CaseSequence, `YYYY-MMM`, KWH) %>%
  rbind(raw_data, .) %>%
  write.xlsx("Residential_Customer_Load_Forecast.xlsx")
```



-----
-----

## Part C – BONUS, optional (part or all), Waterflow_Pipe1.xlsx and Waterflow_Pipe2.xlsx

Part C consists of two data sets.  These are simple 2 columns sets, however they have different time stamps.  Your optional assignment is to time-base sequence the data and aggregate based on hour (example of what this looks like, follows).  Note for multiple recordings within an hour, take the mean.  Then to determine if the data is stationary and can it be forecast.  If so, provide a week forward forecast and present results via Rpubs and .rmd and the forecast in an Excel readable file.   


## Introduction

For this project I have data on water flows from two pipes.  The data are captured with timestamps.  I am to forecast a week forward.  In the instructions for this project I am given the following hint: For multiple recordings within an hour, take the mean.  I will be on the lookout for data under the hour.

## Data Exploration

I will begin by plotting the datasets:

```{r}
pipe_1 <- read_excel("Waterflow_Pipe1.xlsx") %>%
  mutate(`Date Time` = as.POSIXct(`Date Time` * (60 * 60 * 24), origin = "1899-12-30", tz = "GMT"))
pipe_2 <- read_excel("Waterflow_Pipe2.xlsx") %>%
  mutate(`Date Time` = as.POSIXct(`Date Time` * (60 * 60 * 24), origin = "1899-12-30", tz = "GMT"))
pipes <- pipe_1 %>%
  mutate(Pipe = "Pipe 1") %>%
  rbind(mutate(pipe_2, Pipe = "Pipe 2"))
ggplot(pipes, aes(`Date Time`, WaterFlow, color = Pipe)) +
  geom_line() +
  scale_color_brewer(palette = "Set1") +
  facet_wrap(. ~ Pipe) +
  theme(legend.position = "none", axis.title = element_blank())
```

The two pipes are quite different in terms of the data they offer.  Pipe 1 has a median waterflow of about `r round(median(pipe_1$WaterFlow))`, while pipe 2 has a median of `r round(median(pipe_2$WaterFlow))`.  Pipe 1 also has what looks like over a week of data, while pipe 2 has over a month.  Both datasets have 1000 observations.  This suggests that pipe one has more fine grain observations (i.e. multiple observations in a hour or minute) while pipe 2's data captures a longer period of time.  That would explain the difference in median and the length of time covered by the respective data sets.  This also is hinted at in the instructions.

### Closer Look at Pipe 1

I will look at the head and tail of pipe 1's data to get a sense of what's included:

```{r}
head(pipe_1) %>% kable() %>% kable_styling()
```

There is multiple observations per minute which confirms my suspicions.  I will need to create the averages per minute as directed.

### Closer Look at Pipe 2

I will also take a look at pipe 2's data to see if there is anything I need to be aware of:

```{r}
head(pipe_2) %>% kable() %>% kable_styling()
```

Some of the timestamps are just a bit off from the hour mark.  I will need to clean that up.

## Data Cleanup

### Pipe 1

I will clean up the pipe 1 data by creating the averages per minute.

```{r}
pipe_1 <- pipe_1 %>%
  mutate(Date = as.Date(`Date Time`),
         Time = paste0(format(`Date Time`, "%H"), ":00:00")) %>%
  group_by(Date, Time) %>%
  summarise(WaterFlow = mean(WaterFlow)) %>%
  ungroup() %>%
  mutate(`Date Time` = as.POSIXct(paste(as.character(Date), Time)), format = "%Y-%m-%d %H:%M:%OS") %>%
  select(`Date Time`, WaterFlow)
```

After doing this I have `r nrow(pipe_1)` observations.  

#### Sidebar: Is there any pattern in the usage by hour?

I want to pause right here to see if there is any pattern in the the water flow by hour.

```{r}
pipe_1 %>%
  mutate(Time = format(`Date Time`, "%H")) %>%
  ggplot(aes(Time, WaterFlow)) +
  geom_boxplot() +
  ggtitle("Pipe #1 Water Flow by Hour") +
  theme(axis.title = element_blank())
```

There is not a discernable pattern in the water flow by hour.  With that exploration done I will convert the data to a time series object and plot it:

```{r}
pipe_1_ts <- ts(pipe_1$WaterFlow, start = c(2015, 10, 23), frequency = 24)
is_stationary <- function(ts) {
  results <- kpss.test(ts)
  if (results$p.value > 0.05) {
    "data IS stationary"
  } else {
    "data is NOT stationary"
  }
}
 
ggtsplot <- function(ts, title) {
  # A ggplot2 version of tsdisplay()
  # Args:
  #    ts (Time-Series): The time series we want to plot
  #    title (str): The title of the graph
  grid.arrange(
    autoplot(ts) +
      scale_y_continuous(labels = comma) +
      ggtitle(paste0(title, " (", is_stationary(ts), ")")) +
      theme(axis.title = element_blank(), axis.text.x = element_blank(), axis.ticks.x = element_blank()),
    grid.arrange(
      ggAcf(ts) + ggtitle(element_blank()),
      ggPacf(ts) + ggtitle(element_blank()), ncol = 2)
    , nrow = 2)
}
ggtsplot(pipe_1_ts, "Pipe #1 Waterflow")
```

This time series falls within the ACF and the data is stationary.  With stationary data I am done with this data set and can focus on pipe #2.

### Pipe 2

I will clean up pipe 2's `Date Time` field by rounding it to the nearest hour.

```{r}
pipe_2 <- pipe_2 %>%
  mutate(Date = as.Date(`Date Time`),
         Time = format(round(`Date Time`, units = "hours"), format = "%H:%M"),
         `Date Time` = as.POSIXct(paste(as.character(Date), Time)), format = "%Y-%m-%d %H:%M:%OS") %>%
  select(`Date Time`, WaterFlow)
```

#### Sidebar: Is there any pattern in the usage by hour?

I again want to see if there is a pattern in this pipe's usage by hour:

```{r}
pipe_2 %>%
  mutate(Time = format(`Date Time`, "%H")) %>%
  ggplot(aes(Time, WaterFlow)) +
  geom_boxplot() +
  ggtitle("Pipe #2 Water Flow by Hour") +
  theme(axis.title = element_blank())
```


Again there is not a discernable pattern in the water flow by hour.  Now I will convert the data to a time series object and plot it:

```{r}
pipe_2_ts <- ts(pipe_2$WaterFlow, start = c(2015, 10, 23, 1), frequency = 24)
ggtsplot(pipe_2_ts, "Pipe #2 Water Flow")
```

The data is stationary so we can begin forecasting.

## Model Creation 

### A Note on Process

I will develop a variety of models using different methods and will validate their performance using cross validation. The model that minimizes the error will be the model of choice.

### Pipe 1 Candidates

#### STL + ETS

I will begin with a STL decomposition based model.

```{r}
h <- 24 * 7
pipe_1_stl_ets_fit <- pipe_1_ts %>%
  stlf(h = h, s.window = 24, robust = TRUE, method = "ets")
checkresiduals(pipe_1_stl_ets_fit)
```

This model appears to have preformed fairly well.  The residuals are normal and there's only one minor ACF spike.  I'll see how this preforms in the cross validation stage.

#### STL + ARIMA

Next I will create an ARIMA model on the STL decomposed time series.

```{r}
pipe_1_stl_arima_fit <- pipe_1_ts %>%
  stlf(h = h, s.window = 24, robust = TRUE, method = "arima")
checkresiduals(pipe_1_stl_arima_fit)
```

That's strange.  It is a ARIMA(0,0,0) model.  That is a white-noise model.  Let's see if this is an artifact of the STL decomposition or if we get a similar result training the ARIMA model.

#### ARIMA

Last of all I will try an ARIMA model using the `auto.arima` function to define the (p,d,q) parameters.

```{r}
pipe_1_arima_fit <- auto.arima(pipe_1_ts)
checkresiduals(pipe_1_arima_fit)
```

Again the best model was an ARIMA(0,0,0) model!  This suggests there is no reliable way to forecast the waterflow.

### Pipe 2 Candidates

#### STL + ARIMA

I will first check with a STL decomposition and ARIMA to see if it results in a white noise model.

```{r}
pipe_2_stl_arima_fit <- pipe_2_ts %>%
  stlf(h = h, s.window = 24, robust = TRUE, method = "arima")
checkresiduals(pipe_2_stl_arima_fit)
```

It did.  Let's see what kind of model `auto.arima` comes up with.

#### ARIMA

```{r}
pipe_2_arima_fit <- auto.arima(pipe_2_ts)
checkresiduals(pipe_2_arima_fit)
```

We have another white noise model.  The water flow from pipe 2 cannot be forecasted reliably.

## Summary

White noise models were recommended when modeling the water flow of both pipes using the `auto.arima` function.  This suggests that there is not a reliable way to model the waterflow.

