---
title: "Homework 5 Predictive analytics"
author: "Salma Elshahawy"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Homework 5 Predictive analytics}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
[Github repo](https://github.com/salma71/MSDS_spring2021/tree/master/data_624) | [portfolio](https://salma71.github.io/) | [Blog](https://salmaeng71.medium.com/)

```{r setup, include = FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	fig.height = 3,
	fig.width = 7,
	message = FALSE,
	warning = FALSE,
	collapse = TRUE,
	comment = "#>"
)
library(fpp2)
library(tidyverse)
theme_set(theme_classic())
```


## problem 7.1

Consider the `pigs` series - the number of pigs slaughtered in Victoria each month.

  **a. Use the `ses()` function in R to find the optimal values of alpha and l0, and generate forecasts for the next four months.**
  
```{r}
head(pigs)
```

```{r}
pigs_fit <- ses(pigs, h = 5)
summary(pigs_fit)
round(accuracy(pigs_fit),2)
```

**After applying the `ses()` we got an `alpha` = 0.2971, `initial state l` = 77260.056**


  b. Compute a 95% prediction interval for the first forecast using ^y ± 1.96s where s is the standard deviation of the residuals. Compare your interval with the interval produced by R.
  
**The forecast point would be `98816.41`, We would get the standard deviation of it's residuals to be able to calculate required confidence interval**

```{r}
first_fct <- 98816.41 
first_sd <- sd(residuals(pigs_fit))
paste('Standard deviation of the residual for the given point is: ', first_sd)
lo_95 <- first_fct - 1.96*first_sd
paste('The lower bound of 95 confidence interval is: ', lo_95)
hi_95 <- first_fct + 1.96*first_sd
paste('The upper bound of 95 confidence interval is: ', hi_95)

```

**If we compare the two results (manual and obtained using ses), it appears that it slightly difference. The high bound by`ses()` is pretty wider; however the lower bound showed a tighter range than obtain manually.**

---------

  
## problem 7.5

Data set `books` contains the daily sales of paperback and hardcover books at the same store. The task is to forecast the next four days' sales for paperback and hardcover books.

  a. Plot the series and discuss the main features of the data.

```{r}
autoplot(books) + 
  ggtitle('Sales of Books at a Store') +
  xlab('Day') +
  ylab('Books Sold')
```
**The plot revealed that the `book` series is cyclic with an increasing trend for both hardcover and paperback versions. However, there is no evidence of seasonality.**

  b. Use the `ses()` function to forecast each series, and plot the forecasts.
  
**I used the `ses()` with argument `h = 4` to get point of forecasts for both books, hardcover and papaerback.**
  
```{r}
sesfitp <- ses(books[,1], h = 4)
sesfith <- ses(books[,2], h = 4)
summary(sesfitp)
```

```{r fig.height=4}
autoplot(sesfitp) +
  autolayer(fitted(sesfitp), series='Fitted') +
  ggtitle('SES Fit and Forecast of Paperback Sales') +
  xlab('Day') +
  ylab('Books Sale')
```

```{r}
summary(sesfith)
```

```{r fig.height=4}
autoplot(sesfith) +
  autolayer(fitted(sesfith), series='Fitted') +
  ggtitle('SES Fit and Forecast of Hardcover Sales') +
  xlab('Day') +
  ylab('Books Sale')
```

 
  c. Compute the RMSE values for the training data in each case.
  
```{r}
paste0('The RMSE for the hardcover training data is: ', round(max(accuracy(sesfith)),2))
```

```{r}
paste0('The RMSE for the hardcover training data is: ', round(max(accuracy(sesfitp)),2))
```

-------

## problem 7.6

We will continue with the daily sales of paperback and hardcover books in data set books.

  a. Now apply Holt's linear method to the paperback and hardback series and compute four-day forecasts in each case.
  
** For the paperback**  

```{r}
holtfitp <- holt(books[,1], h=4)
forecast(holtfitp)
```

**For the hardcover**  

```{r}
holtfith <- holt(books[,2], h=4)
forecast(holtfith)
```

  b. Compare the RMSE measures of Holt's method for the two series to those of simple exponential smoothing in the previous question. (Remember that Holt's method is using one more parameter than SES.) Discuss the merits of the two forecasting methods for these data sets.
  
```{r}
round(accuracy(holtfitp), 2)
```

```{r}
round(accuracy(holtfith), 2)
```
  
**For the paperback, the RMSE is 31.14. This is `33.64-31.14 = 2.5` improvement.**

**For the hardcover, the RMSE is 27.19. This is `31.93-27.19 = 4.74` improvement.**

**So in terms of prediction accuracy in the training set, Holt’s method is better than the simple exponential smoothing. Holt’s method takes into account the trend element of a time series, while the SES does not have a trend element. The books dataset clearly exhibit a upward trend. Therefore, Holt’s method is more appropriate.**

  
  c. Compare the forecasts for the two series using both methods. Which do you think is best?
  
```{r}
sesfitp <- ses(books[,1], h=4)
sesfith <- ses(books[,1], h=4)

autoplot(books[,1]) +
  autolayer(holtfitp, series='Holts Method', PI=F) +
  autolayer(sesfitp, series='Simple ETS', PI=F) +
  ggtitle('Paperback Sales') +
  xlab('Day') +
  ylab('Books Sales') +
  guides(colour=guide_legend(title="Forecast"))
```

```{r}
autoplot(books[,2]) +
  autolayer(holtfith, series='Holts Method', PI=F) +
  autolayer(sesfith, series='Simple ETS', PI=F) +
  ggtitle('Hardcover Sales') +
  xlab('Day') +
  ylab('Books Sales') +
  guides(colour=guide_legend(title="Forecast"))
```
**The simple ETS method will forecast a constant value without taking into account the trend, while Holt’s method does.**
  
  d. Calculate a 95% prediction interval for the first forecast for each series, using the RMSE values and assuming normal errors. Compare your intervals with those produced using ses and holt.

```{r}
library(kableExtra)
rmsep <- 31.14
ptholtp <- 209.4668
ptsesp <- 207.1097
lowerp <- ptholtp - 1.96 * rmsep
upperp <- ptholtp + 1.96 * rmsep
holtlowerp <- 143.9130
holtupperp <- 275.0205
seslowerp <- 138.8670
sesupperp <- 275.3523

rmseh <- 27.19
ptholth <- 250.1739
ptsesh <- 239.5601
lowerh <- ptholth - 1.96 * rmseh
upperh <- ptholth + 1.96 * rmseh
holtlowerh <- 192.9222
holtupperh <- 307.4256
seslowerh <- 174.7799
sesupperh <- 304.3403


df <- data.frame(c(ptholtp, lowerp, upperp), c(ptholtp, holtlowerp, holtupperp), c(ptsesp, seslowerp, sesupperp), c(ptholth, lowerh, upperh), c(ptholth, holtlowerh, holtupperh), c(ptsesh, seslowerh, sesupperh))
df[4,] <- df[3,] - df[2,]
colnames(df) <- c('Calculated', 'R - holt', 'R - ses', 'Calculated', 'R - holt', 'R - ses')
row.names(df) <- c('Point Forecast', 'Lower 95%', 'Upper 95%', 'Interval Range')

kable(df) %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% 
  add_header_above(c(' ', 'Paperback Forecast' = 3, 'Hardcover Forecost' = 3))
```

**I created a comparasion table to compare the RMSE over the two methods for the two types of book. It seems that the interval calculated using RMSE is slightly narrower than calculated using `holt()` and `ses()` method.**

------

## problem 7.7

For this exercise use data set eggs, the price of a dozen eggs in the United States from 1900–1993. Experiment with the various options in the holt() function to see how much the forecasts change with damped trend, or with a Box-Cox transformation. Try to develop an intuition of what each argument is doing to the forecasts.

> [Hint: use h=100 when calling holt() so you can clearly see the differences between the various options when plotting the forecasts.]

Which model gives the best RMSE?

**Below, I experiment with the default `holt()` and the 3 options of the function. The `damped=TRUE` will use a damped trend. The `exponential=TRUE` will use an exponential trend. The `lambda="auto"` will turn on the Box-Cox transformation for the data, and I also use `biasadj=TRUE` to get the mean forecast (instead of median).**

```{r}
default <- holt(eggs, h=100)
damped <- holt(eggs, h=100, damped = T)
exponential <- holt(eggs, h=100, exponential = T)
lambda <- holt(eggs, h=100, lambda = 'auto', biasadj = T)
da_ex <- holt(eggs, h=100, exponential = T, damped = T)
da_la <- holt(eggs, h=100, damped = T, lambda = 'auto', biasadj = T)

autoplot(eggs) +
  autolayer(default, series='Default', PI=F) +
  autolayer(damped, series='Damped', PI=F) +
  autolayer(exponential, series='Exponential', PI=F) +
  autolayer(lambda, series='Box-Cox Transformed', PI=F) +
  autolayer(da_ex, series='Damped & Exponential', PI=F) +
  autolayer(da_la, series='Damped & Box-Cox', PI=F) +
  ggtitle('Forecast of US Eggs Prices') +
  xlab('Year') +
  ylab('Price of Dozen Eggs')

```
**The forecast value is a straight line and can go to negative. The damped trend seems to damp the forecast very quickly into a flat, horizontal line. The exponential trend forecast appears to be very close to the Box-Cox transformed prediction. And they both shows much more gentle decline than the damped trend method.**

**I also tried 2 combination of the options. The damped and exponential options combine will produce a line similar to damped line. It seems the damped effect out-weights the exponential effect. The damped and Box-Cox transformed produces an increase forecast - which clearly does not make sense.**

**Below are the accuracy for the forecasts aggregated in a dataframe table:**

```{r}
df <- rbind(accuracy(default), accuracy(damped), accuracy(exponential), accuracy(lambda), accuracy(da_ex), accuracy(da_la))
row.names(df) <- c('Default', 'Damped', 'Exponential', 'Box-Cox', 'Damped & Exponential', 'Damped & Box-Cox')
kable(df) %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
```

**As demonstrated from the table above, the Box-Cox transformed using `holt()` method has the lowest RMSE, 26.38.**

-------

## Problem 7.8

Recall your retail time series data (from Exercise 3 in Section 2.10).

  a. Why is multiplicative seasonality necessary for this series?

```{r}
retaildata <- readxl::read_excel("retail.xlsx", skip=1)
myts <- ts(retaildata[,"A3349873A"],
  frequency=12, start=c(1982,4))
```

```{r}
autoplot(myts) +
  ggtitle('Turnover; New South Wales; Other retailing') +
  ylab('Turnover')
```
**the data showed that the seasonality indices increased when the retail sales increased. Multiplicative seasonality can reflect the situation in the model, while additive seasonality can not.**
  
  b. Apply Holt-Winters’ multiplicative method to the data. Experiment with making the trend damped.
  
```{r}
ets_AAM_retail <- hw(myts, seasonal = "multiplicative")
ets_AAdM_retail <- hw(myts, seasonal = "multiplicative", damped = TRUE)

autoplot(ets_AAM_retail)
```

```{r}
autoplot(ets_AAdM_retail)
```

```{r}
autoplot(myts) +
  autolayer(ets_AAM_retail, PI=F, series='Not damped') +
  autolayer(ets_AAdM_retail, PI=F, series='Damped Trend') +
  guides(colour=guide_legend(title="Forecast")) +
  ggtitle("Turnover Forecast - Holt-Winter's Multiplicative Method") +
  ylab('Turnover')
```


**From the plots, it seems that the seasonal variation increases with time. Therefore, multiplicative seasonality is more suitable. The forecasts increased more slowly when damped option was used than it wasn’t used.**
  
  c. Compare the RMSE of the one-step forecasts from the two methods. Which do you prefer?
  
```{r}
error_ets_AAM_retail <- tsCV(myts, hw, h = 1, seasonal = "multiplicative")
error_ets_AAdM_retail <- tsCV(myts, hw, h = 1, seasonal = "multiplicative", damped = TRUE)

sqrt(mean(error_ets_AAM_retail^2, na.rm = TRUE))
```

```{r}
sqrt(mean(error_ets_AAdM_retail^2, na.rm = TRUE))
```

**When the RMSE values were compared, they were almost same. Therefore I prefer damped model because it will prohibit the limitless increase of sales forecast.**
  
  d. Check that the residuals from the best method look like white noise.
  
```{r}
checkresiduals(ets_AAdM_retail)
```

**This seems to be indeed white noise, with occasional spikes.**

  e. Now find the test set RMSE, while training the model to the end of 2010. Can you beat the seasonal naïve approach from Exercise 8 in Section 3.7?
  
**I will utilize three methods, Seasonal Naive, Holt-Winter’s Multiplicative Trend (Holt-Winter 1), and Holt-Winter’s Additive Trend, with Box-Cox Transform (Holt-Winter 2)**
  
  - **Holt-Winters’ method with damped option:**
  
```{r}
ts_retail_train <- window(myts, end = c(2010, 12))
ts_retail_test <- window(myts, start = 2011)
```

```{r}
retail_train_ets_AAdM <- hw(ts_retail_train, h = 36, seasonal = "multiplicative", damped = TRUE)

autoplot(retail_train_ets_AAdM)
```

```{r}
accuracy(retail_train_ets_AAdM, ts_retail_test)
```


  - **Holt-Winters’ method without damped option:**

```{r}
retail_train_ets_AAM <- hw(ts_retail_train, h = 36, seasonal = "multiplicative")

autoplot(retail_train_ets_AAM)
```
```{r}
accuracy(retail_train_ets_AAM, ts_retail_test)
```

  **1. When I used Holt-Winters’ method with damped option, I couldn’t beat seasonal naive approach.**
  **2. When I used Holt-Winters’ method without damped option, I could get better accuracy than when I used the option but it still couldn’t beat the seasonal naive approach.**
  **3. In this case, damped Holt-Winters’ method was worse than Holt-Winters’ method because the actual sales amount in the forecast horizon was exponentially increasing, not damping.**
  **4. I think that this case reflects the fact that the assumption behind the chosen forecast method should be right to forecast more accurately.**

-------

## problem 7.9

For the same retail data, try an STL decomposition applied to the Box-Cox transformed series, followed by ETS on the seasonally adjusted data. How does that compare with your best previous forecasts on the test set?

**The training set is first Box-Cox transformed, and then decomposed using STL.**

```{r}
train <- ts(as.vector(myts), start=c(1982,4), end=c(2010,12), frequency = 12)
lambda <- BoxCox.lambda(train)
paste('Best lambda for Box-Cox Transformation is found to be:', lambda)
```

```{r fig.height=4}
train.bc <- BoxCox(train, lambda)
fit.stl <- stl(train.bc, s.window='periodic', robust=T)

autoplot(fit.stl) +
  ggtitle('STL Decomposition')
```

```{r}
train.bc.seadj <- train.bc - fit.stl$time.series[,'seasonal']

autoplot(train.bc, series='Unadjusted Data') +
  autolayer(train.bc.seadj, series='Seasonally Adjusted') +
  ylab('')
```
**I fit the seasonally adjusted data using ETS, and let ETS automatically search for best fit.**

```{r}
fit.ets <- ets(train.bc.seadj)
summary(fit.ets)
```

**The ETS(M,A,N), with multiplicative error, additive trend, and no seasonal component.**

**I then use this to make a forecast on the test set. The forecast is then back transformed using `InvBoxCox()`.**

```{r}
fc <- forecast(fit.ets, h=36)$mean
fc <- InvBoxCox(fc, lambda=lambda)
fc
```

```{r}
autoplot(ts_retail_test, series='Ground Truth') +
  autolayer(fc, series='Forecast') +
  ylab('')
```

**Since there is no seasonal component, the forecast is a straight line trend. The RMSE is found to be:**


```{r}
sqrt(mean(fc^2, na.rm = TRUE))
```


















  
