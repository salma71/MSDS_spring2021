---
title: "Homework 8 Predictive analytics"
author: "Salma Elshahawy"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Homework 8 Predictive analytics}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
[Github repo](https://github.com/salma71/MSDS_spring2021/tree/master/data_624) | [portfolio](https://salma71.github.io/) | [Blog](https://salmaeng71.medium.com/)

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
library(tidyverse)
library(caret)
library(kableExtra)
library(glmnet)
library(pls)
library(corrplot)
library(tidyr)
library(RANN)
library(RColorBrewer)
theme_set(theme_classic())
```

## Problem  7.2

Friedman (1991) introduced several benchmark data sets created by simulation.  On of these simulations used the following nonlinear equations to create data:

$y = 10 sin(\pi x_1x_2) + 20(x_3- 0.5)^2 + 10x_4 + 5x_5 + N(0, \sigma^2)$

where the $x$ values are random variables uniformly distributed between $[0,1]$ (there are also 5 other non-informative variables also created in the simulation).  The package `mlbench` contains a function called `mlbench.friedman1` that simulates these data:

```{r fig.height=7, fig.width=10}
library(mlbench)
set.seed(200)
trainingData <- mlbench.friedman1(200, sd = 1)
## We convert the 'x' data from a matrix to a data frame
## One reason is that this will five the columns names.
trainingData$x <- data.frame(trainingData$x)
## Look at the data using
featurePlot(trainingData$x, trainingData$y)
## or other methods.
## This creates a list with a vector 'y' and a matrix
## of predictors 'x'.  Also simulate a large test set to
## estimate the true error rate with good precision:
testData <- mlbench.friedman1(5000, sd = 1)
testData$x <- data.frame(testData$x)
```

Tune several models on these data.  For example:

### KNN Model

```{r knn_model}
library(caret)
knnModel <- train(x = trainingData$x, 
                  y = trainingData$y,
                  method = "knn",
                  preProcess = c("center", "scale"),
                  tuneLength = 10)
knnModel
```

```{r}
knnPred <- predict(knnModel, newdata = testData$x)
## The function 'postResample' can be used to get the test set
## performance values
postResample(pred = knnPred, obs = testData$y)
```

Which models appear to give the best performance?  Does MARS select the informative predictors (those named X1-X5)?

### MARS Model

```{r mars_model}
MARS_grid <- expand.grid(.degree = 1:2, .nprune = 2:15)
MARS_model <- train(x = trainingData$x, 
                  y = trainingData$y,
                  method = "earth",
                  tuneGrid = MARS_grid,
                  preProcess = c("center", "scale"),
                  tuneLength = 10)
MARS_model
```

**The optimal MARS model minimized the RMSE when the nprune = `r MARS_model$bestTune$nprune`  and the degree = `r MARS_model$bestTune$degree`.**

```{r}
MARS_predictions <- predict(MARS_model, newdata = testData$x)
postResample(pred = MARS_predictions, obs = testData$y)
```

**The RMSE of the MARS model is a lot lower than the KNN model.  Let's see what variables are important.**

```{r}
varImp(MARS_model)
```

**The MARS model picks up the X1-X5 varaibles.**


### SVM Model

```{r svm_model}
SVM_model <- train(x = trainingData$x,
                   y = trainingData$y,
                   method = "svmRadial",
                   preProcess = c("center", "scale"),
                   tuneLength = 10,
                   trControl = trainControl(method = "cv"))
SVM_model
```

**The optimal SVM mdoel has a $\sigma$ of `r SVM_model$bestTune$sigma` and an C of `r SVM_model$bestTune$C`.**

```{r}
SVM_predictions <- predict(SVM_model, newdata = testData$x)
postResample(pred = SVM_predictions, obs = testData$y)
```

```{r}
varImp(SVM_model)
```

**The SVM picked up the important varaibles.**

### Neural Network Model

```{r neural_network_model}
nnet_grid <- expand.grid(.decay = c(0, 0.01, .1), .size = c(1:10), .bag = FALSE)
nnet_maxnwts <- 5 * (ncol(trainingData$x) + 1) + 5 + 1
nnet_model <- train(x = trainingData$x,
                    y = trainingData$y,
                    method = "avNNet",
                    preProcess = c("center", "scale"),
                    tuneGrid = nnet_grid,
                    trControl = trainControl(method = "cv"),
                    linout = TRUE,
                    trace = FALSE,
                    MaxNWts = nnet_maxnwts,
                    maxit = 500)
nnet_model
```

**The best neural network has a size = `r nnet_model$bestTune$size` and a decay of `r nnet_model$bestTune$decay`.**

```{r}
nnet_predictions <- predict(nnet_model, newdata = testData$x)
postResample(pred = nnet_predictions, obs = testData$y)
```

```{r}
varImp(nnet_model)
```

**The top 5 variables are the ones we want to see listed.**

### Summary

```{r}
results <- data.frame(t(postResample(pred = knnPred, obs = testData$y))) %>% 
  mutate("Model" = "KNN")
results <- data.frame(t(postResample(pred = MARS_predictions, obs = testData$y))) %>%
  mutate("Model"= "MARS") %>%
  bind_rows(results)
results <- data.frame(t(postResample(pred = SVM_predictions, obs = testData$y))) %>%
  mutate("Model"= "SVM") %>%
  bind_rows(results)
results <- data.frame(t(postResample(pred = nnet_predictions, obs = testData$y))) %>%
  mutate("Model"= "Neural Network") %>%
  bind_rows(results)
results %>%
  select(Model, RMSE, Rsquared, MAE) %>%
  arrange(RMSE) %>%
  kable() %>%
  kable_styling()
```

**The MARS model preformed the best and identified the right variables as the important ones.  The $R^2$ on it is extremely high.  I'm impressed by it's performance on the test set!**

## Problem 7.5

Exercise 6.3 describes data for a chemical manufacturing process.  Use the same data imputation, data splitting, and pre-processing steps as before and traing several nonlinear regression models.

**I will run the data through the models in the chapter trying to keep each model as close to the original linear model in terms of the parameters.**

```{r}
library(AppliedPredictiveModeling)
data(ChemicalManufacturingProcess)
# Make this reproducible
set.seed(42)
knn_model <- preProcess(ChemicalManufacturingProcess, "knnImpute")
df <- predict(knn_model, ChemicalManufacturingProcess)
df <- df %>%
  select_at(vars(-one_of(nearZeroVar(., names = TRUE))))
in_train <- createDataPartition(df$Yield, times = 1, p = 0.8, list = FALSE)
train_df <- df[in_train, ]
test_df <- df[-in_train, ]
pls_model <- train(
  Yield ~ ., data = train_df, method = "pls",
  center = TRUE,
  scale = TRUE,
  trControl = trainControl("cv", number = 10),
  tuneLength = 25
)
pls_model
pls_predictions <- predict(pls_model, test_df)
results <- data.frame(t(postResample(pred = pls_predictions, obs = test_df$Yield))) %>%
  mutate("Model"= "PLS")
```

### Part A

Which nonlinear regression model give the optimal resampling and test set performance?

#### KNN Model

```{r}
knn_model <- train(
  Yield ~ ., data = train_df, method = "knn",
  center = TRUE,
  scale = TRUE,
  trControl = trainControl("cv", number = 10),
  tuneLength = 25
)
knn_model
knn_predictions <- predict(knn_model, test_df)
results <- data.frame(t(postResample(pred = knn_predictions, obs = test_df$Yield))) %>%
  mutate("Model"= "KNN") %>% rbind(results)
```

#### MARS Model

```{r}
MARS_grid <- expand.grid(.degree = 1:2, .nprune = 2:15)
MARS_model <- train(
  Yield ~ ., data = train_df, method = "earth",
  tuneGrid = MARS_grid,
  # If the following lines are uncommented, it throws an error
  #center = TRUE,
  #scale = TRUE,
  trControl = trainControl("cv", number = 10),
  tuneLength = 25
)
MARS_model
MARS_predictions <- predict(MARS_model, test_df)
results <- data.frame(t(postResample(pred = MARS_predictions, obs = test_df$Yield))) %>%
  mutate("Model"= "MARS") %>% rbind(results)
```

#### SVM Model

```{r}
SVM_model <- train(
  Yield ~ ., data = train_df, method = "svmRadial",
  center = TRUE,
  scale = TRUE,
  trControl = trainControl(method = "cv"),
  tuneLength = 25
)
SVM_model
SVM_predictions <- predict(SVM_model, test_df)
results <- data.frame(t(postResample(pred = SVM_predictions, obs = test_df$Yield))) %>%
  mutate("Model"= "SVM") %>% rbind(results)
```

#### Neural Network Model

```{r}
nnet_grid <- expand.grid(.decay = c(0, 0.01, .1), .size = c(1:10), .bag = FALSE)
nnet_maxnwts <- 5 * ncol(train_df) + 5 + 1
nnet_model <- train(
  Yield ~ ., data = train_df, method = "avNNet",
  center = TRUE,
  scale = TRUE,
  tuneGrid = nnet_grid,
  trControl = trainControl(method = "cv"),
  linout = TRUE,
  trace = FALSE,
  MaxNWts = nnet_maxnwts,
  maxit = 500
)
nnet_model
nnet_predictions <- predict(nnet_model, test_df)
results <- data.frame(t(postResample(pred = nnet_predictions, obs = test_df$Yield))) %>%
  mutate("Model"= "Neural Network") %>% rbind(results)
```

#### Summary

```{r}
results %>%
  select(Model, RMSE, Rsquared, MAE) %>%
  arrange(RMSE) %>%
  kable() %>%
  kable_styling()
```

**The SVM Model was the best non-linear model.**

### Part B

Which predictors are most important in the optimal nonlinear regression model?  Do either the biological or process variables dominate the list?  How do the top ten important predictors compare to the top ten predictors from the optimal linear model?

```{r}
varImp(SVM_model, 10)
```

```{r}
varImp(pls_model, 10)
```


**The SVM model was very similar to the PLS model.  In either case the manufacuturing process variables dominate the list. The SVM model found `BiologicalMaterial12` to be important and didn't find `BiologicalMaterial02` to be important.**

### Part C

Explore the relationships between the top predictors and the response for the predictors that are unique to the optimal nonlinear regression model. Do these plots reveal intuition about the biological or process predictors and their relationship with yield?


```{r}
ggplot(train_df, aes(BiologicalMaterial12, Yield)) +
  geom_point()
```

**This indicates a positive relationship between the `BiologicalMaterial02` and the yeild, although it seems that there is a sweet spot, or a point of diminishing marginal returns on the material as the highest yeilds are not the furthest to the right on the graph.**